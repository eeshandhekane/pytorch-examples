{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "\u0001() method: bad call flags",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mSystemError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-2c0ab017ac8f>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Dependencies\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/mila-studies/phd/dev/pytorch-examples/venv/lib/python3.9/site-packages/torch/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    194\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mUSE_GLOBAL_DEPS\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    195\u001B[0m         \u001B[0m_load_global_deps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 196\u001B[0;31m     \u001B[0;32mfrom\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_C\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    197\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    198\u001B[0m \u001B[0;31m# Appease the type checker; ordinarily this binding is inserted by the\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mSystemError\u001B[0m: \u0001() method: bad call flags"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Linear space.\n",
    "x = torch.linspace(start=0, end=2*np.pi, steps=100, requires_grad=True)  # Or, `.requires_grad_()`\n",
    "# Check the gradient function that is remembered.\n",
    "print('x: \\n{}'.format(x))\n",
    "y = torch.sin(input=x)\n",
    "print('y: \\n{}'.format(y))\n",
    "z = y * 2\n",
    "print('z: \\n{}'.format(z))\n",
    "w = z + 2\n",
    "print('w: \\n{}'.format(w))\n",
    "# Composed functions.\n",
    "print('*'*79)\n",
    "x = torch.linspace(start=0, end=2*np.pi, steps=100).requires_grad_()\n",
    "print('x: \\n{}'.format(x))\n",
    "w = torch.sin(x)*2 + 2\n",
    "print('w: \\n{}'.format(w))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Branched computations.\n",
    "x = torch.linspace(start=0, end=2*np.pi, steps=100, requires_grad=True)\n",
    "y = torch.sin(input=x)\n",
    "z = y * 2\n",
    "w = z + 2\n",
    "u = z + 2\n",
    "v = z ** 3\n",
    "print('w: \\n{}'.format(w))\n",
    "print('u: \\n{}'.format(u))\n",
    "print('v: \\n{}'.format(v))\n",
    "print('w == u: {}'.format(\n",
    "    torch.all(torch.eq(input=w, other=u))\n",
    "))\n",
    "print('w is u: {}'.format(w is u))\n",
    "# Autograd expects the final computation to lead to a scalar BY DEFAULT.\n",
    "out = (u + v + w).sum()  # Reduces the answer to a scalar of sum of all elements.\n",
    "print('out: \\n{}'.format(out))\n",
    "print('out.grad_fn: {}'.format(out.grad_fn))  # Checking the grad function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Simple example to visualize gradients.\n",
    "x = torch.linspace(start=0, end=2*np.pi, steps=20, requires_grad=True)\n",
    "x_grad_old = x.grad\n",
    "print('x.grad before `.backward()` call: \\n{}'.format(x.grad))\n",
    "y = torch.sin(input=x)\n",
    "z = y * 2\n",
    "w = z + 2\n",
    "out = w.sum()\n",
    "out.backward()\n",
    "x_grads = x.grad\n",
    "print('x.grad after `.backward()` call: \\n{}'.format(x.grad))\n",
    "x_grad_pred = torch.cos(input=x)*2\n",
    "plt.style.use('dark_background')\n",
    "plt.grid(True)\n",
    "\"\"\"\n",
    "If a tensor requires gradient, then it does not have `.numpy()` method.\n",
    "Thus, the following will not work--\n",
    "plt.plot(x.numpy(), x_grads.numpy(), color='green', label='backpropagated gradients', marker='x', alpha=0.8)\n",
    "plt.scatter(x.numpy(), x_grad_pred.numpy(), color='red', label='predicted backpropagated gradients', marker='o', alpha=0.8)\n",
    "Instead, for every tensor `X`, we have to call `X.detach().numpy()` to extract the answer to numpy.\n",
    "\"\"\"\n",
    "plt.plot(x.detach().numpy(), x_grads.detach().numpy(), color='green', label='backpropagated gradients', marker='x', alpha=0.8)\n",
    "plt.scatter(x.detach().numpy(), x_grad_pred.detach().numpy(), color='red', label='predicted backpropagated gradients', marker='o', alpha=0.8)\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "After the `.backward()` pass, intermediate tensors will NOT have attached gradients.\n",
    "\"\"\"\n",
    "print('gradients at y, z, w: {}, {}, {}'.format(y.grad, z.grad, w.grad))  # ALL OF THESE ARE `None`, as stated above!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Autograd expects the final output to be a scalar in order to call `.backward()` on it.\n",
    "However, this is NOT a compulsion, which will be covered later.\n",
    "\"\"\"\n",
    "x = torch.linspace(start=0, end=2*np.pi, steps=20).requires_grad_()\n",
    "y = torch.sin(input=x)\n",
    "z1 = y * 2\n",
    "z2 = y ** 2\n",
    "z3 = z1 * z2\n",
    "w = y + z3\n",
    "out = w.sum()\n",
    "print('*'*79)  # History for `out`.\n",
    "print('History of out:')\n",
    "print('\\tgrad function: {}'.format(out.grad_fn))\n",
    "print('\\tnext functions to grad: {}'.format(out.grad_fn.next_functions))\n",
    "print('*'*79)  # History for `w`.\n",
    "print('History of w:')\n",
    "print('\\tgrad function: {}'.format(w.grad_fn))\n",
    "print('\\tnext functions to grad: {}'.format(w.grad_fn.next_functions))\n",
    "print('*'*79)  # History for `z1`.\n",
    "print('History of z1:')\n",
    "print('\\tgrad function: {}'.format(z1.grad_fn))\n",
    "print('\\tnext functions to grad: {}'.format(z1.grad_fn.next_functions))\n",
    "print('*'*79)  # History for `z2`.\n",
    "print('History of z2:')\n",
    "print('\\tgrad function: {}'.format(z2.grad_fn))\n",
    "print('\\tnext functions to grad: {}'.format(z2.grad_fn.next_functions))\n",
    "print('*'*79)  # History for `z3`.\n",
    "print('History of z3:')\n",
    "print('\\tgrad function: {}'.format(z3.grad_fn))\n",
    "print('\\tnext functions to grad: {}'.format(z3.grad_fn.next_functions))\n",
    "print('*'*79)  # History for `y`.\n",
    "print('History of y:')\n",
    "print('\\tgrad function: {}'.format(y.grad_fn))\n",
    "print('\\tnext functions to grad: {}'.format(y.grad_fn.next_functions))\n",
    "print('*'*79)  # History for `x`.\n",
    "print('History of x:')\n",
    "print('\\tgrad function: {}'.format(x.grad_fn))\n",
    "print('*'*79+'\\n'+'*'*79)  # Detailed history for `out`.\n",
    "print('Detailed history of out:\\n')\n",
    "print('\\tgrad: {}'.format(out.grad_fn))\n",
    "# <SumBackward0 object at <...>> : `out = w.sum()`\n",
    "print('\\tgrad->next: {}'.format(out.grad_fn.next_functions))\n",
    "# ((<AddBackward0 object at <...>>, 0),) : `w = y + z3`\n",
    "print('\\tgrad->next->next: {}'.format(out.grad_fn.next_functions[0][0].next_functions))\n",
    "# ((<SinBackward object at <...>>, 0), (<MulBackward0 object at <...>>, 0)) : z3 = z1 * z2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}