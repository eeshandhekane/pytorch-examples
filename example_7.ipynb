{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "batch_size: int = 4\n",
    "input_dim: int = 10\n",
    "output_dim: int = 2\n",
    "hidden_dim: int = 5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    r\"\"\"An example model.\n",
    "    \"\"\"\n",
    "\n",
    "    #\n",
    "    def __init__(self):\n",
    "        r\"\"\"The initializer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=input_dim, out_features=hidden_dim, bias=True)\n",
    "        self.act_1 = nn.ReLU()\n",
    "        self.layer_2 = nn.Linear(in_features=hidden_dim, out_features=output_dim, bias=True)\n",
    "\n",
    "    #\n",
    "    def forward(self, x_: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Implements the forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_:\n",
    "            The input tensor.\n",
    "            SHAPE: [*<batch_dims>, input_dim].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out:\n",
    "            The output tensor.\n",
    "            SHAPE: [*<batch_dims>, output_dim].\n",
    "        \"\"\"\n",
    "        out_1 = self.act_1(self.layer_1(x_))\n",
    "        out = self.layer_2(out_1)\n",
    "        return  out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:\n",
      "Model(\n",
      "  (layer_1): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (act_1): ReLU()\n",
      "  (layer_2): Linear(in_features=5, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "print('model:\\n{}'.format(model))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data shape: torch.Size([4, 10])\n",
      "output data shape: torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "input_data = torch.randn(size=(batch_size, input_dim)).requires_grad_()\n",
    "output_data = torch.randn(size=(batch_size, output_dim)).requires_grad_()\n",
    "print('input data shape: {}'.format(input_data.shape))\n",
    "print('output data shape: {}'.format(output_data.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model layer 1 data details:\n",
      "\tdata type:\n",
      "\t\tweight: <class 'torch.nn.parameter.Parameter'>\n",
      "\t\tbias: <class 'torch.nn.parameter.Parameter'>\n",
      "\tdata shape:\n",
      "\t\tweight: torch.Size([5, 10])\n",
      "\t\tbias: torch.Size([5])\n",
      "\tdata attr type:\n",
      "\t\tweight: <class 'torch.Tensor'>\n",
      "\t\tbias: <class 'torch.Tensor'>\n",
      "\tdata attr shape:\n",
      "\t\tweight: torch.Size([5, 10])\n",
      "\t\tbias: torch.Size([5])\n",
      "\tdata:\n",
      "\t\tweight: tensor([[-0.2757, -0.2238,  0.1012,  0.2820,  0.3134, -0.0605, -0.1855,  0.2337,\n",
      "          0.2572,  0.2192],\n",
      "        [-0.3035,  0.1361, -0.1711,  0.3041, -0.0623,  0.0329, -0.3068,  0.2005,\n",
      "         -0.1149,  0.1247],\n",
      "        [-0.3055, -0.2741,  0.1318,  0.0795,  0.0851, -0.0998, -0.0532,  0.2881,\n",
      "         -0.2898, -0.2112],\n",
      "        [ 0.2005, -0.2470, -0.1415,  0.2311, -0.1457, -0.0029, -0.0783,  0.1730,\n",
      "         -0.1699,  0.0685],\n",
      "        [ 0.0116, -0.2378, -0.2751,  0.2066,  0.1237, -0.2924,  0.1969, -0.1509,\n",
      "         -0.3160, -0.0388]])\n",
      "\t\tbias: tensor([-0.1250,  0.3048, -0.2829,  0.2584, -0.1077])\n",
      "\tdata -> numpy:\n",
      "\t\tweight: [[-0.27574196 -0.2237502   0.10123414  0.28199056  0.3133538  -0.0604524\n",
      "  -0.18554692  0.2336922   0.2572054   0.21915177]\n",
      " [-0.30351597  0.13610727 -0.17108148  0.30411258 -0.06234312  0.03293529\n",
      "  -0.30683404  0.20049372 -0.11490548  0.12470675]\n",
      " [-0.3054753  -0.2740935   0.13177234  0.07947832  0.08508229 -0.09983474\n",
      "  -0.05316404  0.28809848 -0.28977525 -0.2111998 ]\n",
      " [ 0.200466   -0.2469626  -0.14149353  0.23113093 -0.14568117 -0.00294152\n",
      "  -0.07833239  0.17303276 -0.16989248  0.06845376]\n",
      " [ 0.01157549 -0.23778424 -0.275059    0.20659062  0.12374073 -0.2923631\n",
      "   0.19686392 -0.15094766 -0.3160093  -0.03877071]]\n",
      "\t\tbias: [-0.12504964  0.3047519  -0.2828687   0.258395   -0.10771084]\n",
      "\tgradients:\n",
      "\t\tweight: None\n",
      "\t\tbias: None\n"
     ]
    }
   ],
   "source": [
    "print('model layer 1 data details:')\n",
    "print('\\tdata type:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    type(model.layer_1.weight), type(model.layer_1.bias)\n",
    "))\n",
    "print('\\tdata shape:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    model.layer_1.weight.shape, model.layer_1.bias.shape\n",
    "))\n",
    "print('\\tdata attr type:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    type(model.layer_1.weight.data), type(model.layer_1.bias.data)\n",
    "))\n",
    "print('\\tdata attr shape:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    model.layer_1.weight.data.shape, model.layer_1.bias.data.shape\n",
    "))\n",
    "print('\\tdata:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    model.layer_1.weight.data, model.layer_1.bias.data\n",
    "))\n",
    "print('\\tdata -> numpy:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    model.layer_1.weight.data.numpy(), model.layer_1.bias.data.numpy()\n",
    "))\n",
    "print('\\tgradients:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    model.layer_1.weight.grad, model.layer_1.bias.grad\n",
    "))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model layer 2 data details:\n",
      "\tdata type:\n",
      "\t\tweight: <class 'torch.nn.parameter.Parameter'>\n",
      "\t\tbias: <class 'torch.nn.parameter.Parameter'>\n",
      "\tdata shape:\n",
      "\t\tweight: torch.Size([2, 5])\n",
      "\t\tbias: torch.Size([2])\n",
      "\tdata attr type:\n",
      "\t\tweight: <class 'torch.Tensor'>\n",
      "\t\tbias: <class 'torch.Tensor'>\n",
      "\tdata attr shape:\n",
      "\t\tweight: torch.Size([2, 5])\n",
      "\t\tbias: torch.Size([2])\n",
      "\tdata:\n",
      "\t\tweight: tensor([[-0.2746, -0.2589, -0.0818,  0.1747, -0.3733],\n",
      "        [-0.0527, -0.0410, -0.2782,  0.4263,  0.3732]])\n",
      "\t\tbias: tensor([0.0305, 0.1637])\n",
      "\tdata -> numpy:\n",
      "\t\tweight: [[-0.2746404  -0.25892842 -0.08181432  0.17469496 -0.3732766 ]\n",
      " [-0.05265009 -0.04097661 -0.27816778  0.42629427  0.37320584]]\n",
      "\t\tbias: [0.030462   0.16367447]\n",
      "\tgradients:\n",
      "\t\tweight: None\n",
      "\t\tbias: None\n"
     ]
    }
   ],
   "source": [
    "print('model layer 2 data details:')\n",
    "print('\\tdata type:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    type(model.layer_2.weight), type(model.layer_2.bias)\n",
    "))\n",
    "print('\\tdata shape:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    model.layer_2.weight.shape, model.layer_2.bias.shape\n",
    "))\n",
    "print('\\tdata attr type:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    type(model.layer_2.weight.data), type(model.layer_2.bias.data)\n",
    "))\n",
    "print('\\tdata attr shape:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    model.layer_2.weight.data.shape, model.layer_2.bias.data.shape\n",
    "))\n",
    "print('\\tdata:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    model.layer_2.weight.data, model.layer_2.bias.data\n",
    "))\n",
    "print('\\tdata -> numpy:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    model.layer_2.weight.data.numpy(), model.layer_2.bias.data.numpy()\n",
    "))\n",
    "print('\\tgradients:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    model.layer_2.weight.grad, model.layer_2.bias.grad\n",
    "))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************************************\n",
      "original model:\n",
      "Parameter containing:\n",
      "tensor([[-0.2757, -0.2238,  0.1012,  0.2820,  0.3134, -0.0605, -0.1855,  0.2337,\n",
      "          0.2572,  0.2192],\n",
      "        [-0.3035,  0.1361, -0.1711,  0.3041, -0.0623,  0.0329, -0.3068,  0.2005,\n",
      "         -0.1149,  0.1247],\n",
      "        [-0.3055, -0.2741,  0.1318,  0.0795,  0.0851, -0.0998, -0.0532,  0.2881,\n",
      "         -0.2898, -0.2112],\n",
      "        [ 0.2005, -0.2470, -0.1415,  0.2311, -0.1457, -0.0029, -0.0783,  0.1730,\n",
      "         -0.1699,  0.0685],\n",
      "        [ 0.0116, -0.2378, -0.2751,  0.2066,  0.1237, -0.2924,  0.1969, -0.1509,\n",
      "         -0.3160, -0.0388]], requires_grad=True)\n",
      "update model:\n",
      "Parameter containing:\n",
      "tensor([[0.7269, 0.0331, 0.4493, 0.7921, 0.7990, 0.2386, 0.0191, 0.7845, 0.4033,\n",
      "         0.7511],\n",
      "        [0.4514, 0.5769, 0.9506, 0.3311, 0.5953, 0.0848, 0.7213, 0.4370, 0.5277,\n",
      "         0.8182],\n",
      "        [0.5869, 0.0522, 0.3129, 0.3035, 0.6376, 0.3869, 0.0780, 0.2773, 0.0634,\n",
      "         0.2156],\n",
      "        [0.3120, 0.5989, 0.7191, 0.0701, 0.2125, 0.1313, 0.2468, 0.0265, 0.1520,\n",
      "         0.2165],\n",
      "        [0.9029, 0.0504, 0.3329, 0.0288, 0.2683, 0.5613, 0.1735, 0.0629, 0.8239,\n",
      "         0.6953]], requires_grad=True)\n",
      "*******************************************************************************\n",
      "numpy data:\n",
      "[[0.97901565 0.04156756 0.60849506 0.8744599  0.1410284  0.5262631\n",
      "  0.3361416  0.40333843 0.54719293 0.36351442]\n",
      " [0.5340483  0.33054462 0.5292621  0.25104508 0.0276592  0.34938732\n",
      "  0.2464705  0.23349811 0.88344705 0.3389564 ]\n",
      " [0.34909928 0.53221995 0.33297485 0.5290638  0.7239616  0.6802007\n",
      "  0.15243734 0.75526845 0.41537178 0.69618434]\n",
      " [0.10788152 0.30547357 0.42250374 0.67949414 0.87584794 0.6380482\n",
      "  0.7228027  0.14945033 0.9096019  0.825524  ]\n",
      " [0.99531215 0.39549133 0.22276354 0.5578199  0.2041336  0.47564492\n",
      "  0.08857651 0.6081488  0.20211579 0.5918161 ]]\n",
      "update model from numpy data:\n",
      "Parameter containing:\n",
      "tensor([[0.9790, 0.0416, 0.6085, 0.8745, 0.1410, 0.5263, 0.3361, 0.4033, 0.5472,\n",
      "         0.3635],\n",
      "        [0.5340, 0.3305, 0.5293, 0.2510, 0.0277, 0.3494, 0.2465, 0.2335, 0.8834,\n",
      "         0.3390],\n",
      "        [0.3491, 0.5322, 0.3330, 0.5291, 0.7240, 0.6802, 0.1524, 0.7553, 0.4154,\n",
      "         0.6962],\n",
      "        [0.1079, 0.3055, 0.4225, 0.6795, 0.8758, 0.6380, 0.7228, 0.1495, 0.9096,\n",
      "         0.8255],\n",
      "        [0.9953, 0.3955, 0.2228, 0.5578, 0.2041, 0.4756, 0.0886, 0.6081, 0.2021,\n",
      "         0.5918]], requires_grad=True)\n",
      "*******************************************************************************\n",
      "model layer 1:\n",
      "\tParameter containing:\n",
      "tensor([[0.9790, 0.0416, 0.6085, 0.8745, 0.1410, 0.5263, 0.3361, 0.4033, 0.5472,\n",
      "         0.3635],\n",
      "        [0.5340, 0.3305, 0.5293, 0.2510, 0.0277, 0.3494, 0.2465, 0.2335, 0.8834,\n",
      "         0.3390],\n",
      "        [0.3491, 0.5322, 0.3330, 0.5291, 0.7240, 0.6802, 0.1524, 0.7553, 0.4154,\n",
      "         0.6962],\n",
      "        [0.1079, 0.3055, 0.4225, 0.6795, 0.8758, 0.6380, 0.7228, 0.1495, 0.9096,\n",
      "         0.8255],\n",
      "        [0.9953, 0.3955, 0.2228, 0.5578, 0.2041, 0.4756, 0.0886, 0.6081, 0.2021,\n",
      "         0.5918]], requires_grad=True)\n",
      "model layer 1 slice:\n",
      "\ttensor([[0.9790, 0.0416],\n",
      "        [0.5340, 0.3305]], grad_fn=<SliceBackward>)\n",
      "model layer 1 grad fn:\n",
      "\tNone\n",
      "model layer 1 slice grad fn:\n",
      "\t<SliceBackward object at 0x13a7f33a0>\n",
      "model layer 1 grad:\n",
      "\tNone\n",
      "model layer 1 slice grad:\n",
      "\tNone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-098f39d44a4d>:19: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "  model.layer_1.weight.grad, model.layer_1.weight[0:2, 0:2].grad\n"
     ]
    }
   ],
   "source": [
    "print('*'*79)\n",
    "print('original model:\\n{}'.format(model.layer_1.weight))\n",
    "model.layer_1.weight.data = torch.Tensor(size=(5, 10)).uniform_()\n",
    "print('update model:\\n{}'.format(model.layer_1.weight))\n",
    "print('*'*79)\n",
    "np_weights = np.random.random(size=(5, 10)).astype(np.float32)\n",
    "print('numpy data:\\n{}'.format(np_weights))\n",
    "model.layer_1.weight.data = torch.from_numpy(np_weights)\n",
    "print('update model from numpy data:\\n{}'.format(model.layer_1.weight))\n",
    "# Operations on non-grad tensors can have grad functions associated with them.\n",
    "print('*'*79)\n",
    "print('model layer 1:\\n\\t{}\\nmodel layer 1 slice:\\n\\t{}'.format(\n",
    "    model.layer_1.weight, model.layer_1.weight[0:2, 0:2]\n",
    "))\n",
    "print('model layer 1 grad fn:\\n\\t{}\\nmodel layer 1 slice grad fn:\\n\\t{}'.format(\n",
    "    model.layer_1.weight.grad_fn, model.layer_1.weight[0:2, 0:2].grad_fn\n",
    "))\n",
    "print('model layer 1 grad:\\n\\t{}\\nmodel layer 1 slice grad:\\n\\t{}'.format(\n",
    "    model.layer_1.weight.grad, model.layer_1.weight[0:2, 0:2].grad\n",
    "))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer:\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.001\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(params=model.parameters(), lr=1e-3)\n",
    "print('optimizer:\\n{}'.format(optimizer))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is model training?: True\n",
      "is model training?: False\n",
      "is model training?: True\n",
      "is model training?: False\n"
     ]
    },
    {
     "data": {
      "text/plain": "Model(\n  (layer_1): Linear(in_features=10, out_features=5, bias=True)\n  (act_1): ReLU()\n  (layer_2): Linear(in_features=5, out_features=2, bias=True)\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting modes by ignoring their return values, which are references to the model.\n",
    "print('is model training?: {}'.format(model.training))\n",
    "model.train(mode=False)\n",
    "print('is model training?: {}'.format(model.training))\n",
    "model.train(mode=True)\n",
    "print('is model training?: {}'.format(model.training))\n",
    "model.eval()\n",
    "print('is model training?: {}'.format(model.training))\n",
    "model.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is model training?: True\n",
      "is model training?: False\n",
      "is model training?: True\n",
      "is model training?: False\n"
     ]
    }
   ],
   "source": [
    "print('is model training?: {}'.format(model.training))\n",
    "model = model.train(mode=False)\n",
    "print('is model training?: {}'.format(model.training))\n",
    "model = model.train(mode=True)\n",
    "print('is model training?: {}'.format(model.training))\n",
    "model = model.eval()\n",
    "print('is model training?: {}'.format(model.training))\n",
    "model = model.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:\n",
      "15.137835502624512\n"
     ]
    }
   ],
   "source": [
    "# One epoch.\n",
    "pred_data = model(input_data)\n",
    "loss = (pred_data - output_data).pow(2).sum()\n",
    "print('loss:\\n{}'.format(loss))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************************************\n",
      "data before backprop\n",
      "\tlayer 1:\n",
      "\t\tweight: tensor([[0.9790, 0.0416, 0.6085, 0.8745, 0.1410, 0.5263, 0.3361, 0.4033, 0.5472,\n",
      "         0.3635],\n",
      "        [0.5340, 0.3305, 0.5293, 0.2510, 0.0277, 0.3494, 0.2465, 0.2335, 0.8834,\n",
      "         0.3390],\n",
      "        [0.3491, 0.5322, 0.3330, 0.5291, 0.7240, 0.6802, 0.1524, 0.7553, 0.4154,\n",
      "         0.6962],\n",
      "        [0.1079, 0.3055, 0.4225, 0.6795, 0.8758, 0.6380, 0.7228, 0.1495, 0.9096,\n",
      "         0.8255],\n",
      "        [0.9953, 0.3955, 0.2228, 0.5578, 0.2041, 0.4756, 0.0886, 0.6081, 0.2021,\n",
      "         0.5918]])\n",
      "\t\tbias: tensor([-0.1250,  0.3048, -0.2829,  0.2584, -0.1077])\n",
      "\tlayer 2:\n",
      "\t\tweight: tensor([[-0.2746, -0.2589, -0.0818,  0.1747, -0.3733],\n",
      "        [-0.0527, -0.0410, -0.2782,  0.4263,  0.3732]])\n",
      "\t\tbias: tensor([0.0305, 0.1637])\n",
      "gradients before backprop\n",
      "\tlayer 1:\n",
      "\t\tweight: None\n",
      "\t\tbias: None\n",
      "\tlayer 2:\n",
      "\t\tweight: None\n",
      "\t\tbias: None\n",
      "*******************************************************************************\n",
      "data after backprop\n",
      "\tlayer 1:\n",
      "\t\tweight: tensor([[0.9790, 0.0416, 0.6085, 0.8745, 0.1410, 0.5263, 0.3361, 0.4033, 0.5472,\n",
      "         0.3635],\n",
      "        [0.5340, 0.3305, 0.5293, 0.2510, 0.0277, 0.3494, 0.2465, 0.2335, 0.8834,\n",
      "         0.3390],\n",
      "        [0.3491, 0.5322, 0.3330, 0.5291, 0.7240, 0.6802, 0.1524, 0.7553, 0.4154,\n",
      "         0.6962],\n",
      "        [0.1079, 0.3055, 0.4225, 0.6795, 0.8758, 0.6380, 0.7228, 0.1495, 0.9096,\n",
      "         0.8255],\n",
      "        [0.9953, 0.3955, 0.2228, 0.5578, 0.2041, 0.4756, 0.0886, 0.6081, 0.2021,\n",
      "         0.5918]])\n",
      "\t\tbias: tensor([-0.1250,  0.3048, -0.2829,  0.2584, -0.1077])\n",
      "\tlayer 2:\n",
      "\t\tweight: tensor([[-0.2746, -0.2589, -0.0818,  0.1747, -0.3733],\n",
      "        [-0.0527, -0.0410, -0.2782,  0.4263,  0.3732]])\n",
      "\t\tbias: tensor([0.0305, 0.1637])\n",
      "gradients after backprop\n",
      "\tlayer 1:\n",
      "\t\tweight: tensor([[-0.0554,  0.0091, -0.0063,  0.0893, -0.0374, -0.0057,  0.1270, -0.1034,\n",
      "         -0.0200, -0.1128],\n",
      "        [ 0.2208, -1.0644,  1.1067,  1.6336,  0.1445, -1.4911, -0.9419, -0.3042,\n",
      "          0.0931, -1.0342],\n",
      "        [-0.5762, -1.0998,  1.1901,  3.2395, -0.3946, -1.8185,  0.8150, -1.9083,\n",
      "         -0.1932, -2.8970],\n",
      "        [ 0.8220,  1.8657, -2.0099, -5.1956,  0.5643,  3.0312, -1.0369,  2.9291,\n",
      "          0.2717,  4.5611],\n",
      "        [ 1.3712, -0.2899,  0.2256, -2.0837,  0.9259,  0.0465, -3.1713,  2.5152,\n",
      "          0.4971,  2.6984]])\n",
      "\t\tbias: tensor([-0.0982, -0.9848, -2.6208,  4.1360,  2.3447])\n",
      "\tlayer 2:\n",
      "\t\tweight: tensor([[-0.1999,  1.3026,  2.1033,  3.9816,  0.4821],\n",
      "        [ 1.5498,  7.0599,  9.7054,  6.5243,  8.6351]])\n",
      "\t\tbias: tensor([-0.0841, 10.0950])\n",
      "*******************************************************************************\n",
      "gradient shapes after backprop\n",
      "\tlayer 1:\n",
      "\t\tweight: torch.Size([5, 10])\n",
      "\t\tbias: torch.Size([5])\n",
      "\tlayer 2:\n",
      "\t\tweight: torch.Size([2, 5])\n",
      "\t\tbias: torch.Size([2])\n",
      "gradient shapes match:\n",
      "\tlayer 1:\n",
      "\t\tweight: True\n",
      "\t\tbias: True\n",
      "\tlayer 2:\n",
      "\t\tweight: True\n",
      "\t\tbias: True\n",
      "weights unchanged before and after backprop:\n",
      "\tlayer 1:\n",
      "\t\tweight: True\n",
      "\t\tbias: True\n",
      "\tlayer 2:\n",
      "\t\tweight: True\n",
      "\t\tbias: True\n"
     ]
    }
   ],
   "source": [
    "# Backprop.\n",
    "print('*'*79)\n",
    "print('data before backprop')\n",
    "print('\\tlayer 1:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    model.layer_1.weight.data, model.layer_1.bias.data\n",
    "))\n",
    "print('\\tlayer 2:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    model.layer_2.weight.data, model.layer_2.bias.data\n",
    "))\n",
    "w1_before, b1_before = model.layer_1.weight.data, model.layer_1.bias.data\n",
    "w2_before, b2_before = model.layer_2.weight.data, model.layer_2.bias.data\n",
    "print('gradients before backprop')\n",
    "print('\\tlayer 1:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    model.layer_1.weight.grad, model.layer_1.bias.grad\n",
    "))\n",
    "print('\\tlayer 2:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    model.layer_2.weight.grad, model.layer_2.bias.grad\n",
    "))\n",
    "loss.backward()\n",
    "print('*'*79)\n",
    "print('data after backprop')\n",
    "print('\\tlayer 1:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    model.layer_1.weight.data, model.layer_1.bias.data\n",
    "))\n",
    "print('\\tlayer 2:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    model.layer_2.weight.data, model.layer_2.bias.data\n",
    "))\n",
    "w1_after, b1_after = model.layer_1.weight.data, model.layer_1.bias.data\n",
    "w2_after, b2_after = model.layer_2.weight.data, model.layer_2.bias.data\n",
    "print('gradients after backprop')\n",
    "print('\\tlayer 1:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    model.layer_1.weight.grad, model.layer_1.bias.grad\n",
    "))\n",
    "print('\\tlayer 2:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    model.layer_2.weight.grad, model.layer_2.bias.grad\n",
    "))\n",
    "print('*'*79)\n",
    "print('gradient shapes after backprop')\n",
    "print('\\tlayer 1:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    model.layer_1.weight.grad.shape, model.layer_1.bias.grad.shape\n",
    "))\n",
    "print('\\tlayer 2:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    model.layer_2.weight.grad.shape, model.layer_2.bias.grad.shape\n",
    "))\n",
    "print('gradient shapes match:')\n",
    "print('\\tlayer 1:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    model.layer_1.weight.grad.shape==model.layer_1.weight.shape, model.layer_1.bias.grad.shape==model.layer_1.bias.shape\n",
    "))\n",
    "print('\\tlayer 2:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    model.layer_2.weight.grad.shape==model.layer_2.weight.shape, model.layer_2.bias.grad.shape==model.layer_2.bias.shape\n",
    "))\n",
    "print('weights unchanged before and after backprop:')\n",
    "print('\\tlayer 1:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    torch.eq(w1_before, w1_after).all(), torch.eq(b1_before, b1_after).all()\n",
    "))\n",
    "print('\\tlayer 2:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    torch.eq(w2_before, w2_after).all(), torch.eq(b2_before, b2_after).all()\n",
    "))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients accumulated correctly:\n",
      "\tlayer 1:\n",
      "\t\tweight: 0.0\n",
      "\t\tbias: 0.0\n",
      "\tlayer 2:\n",
      "\t\tweight: 0.0\n",
      "\t\tbias: 0.0\n",
      "gradients match correctly:\n",
      "\tlayer 1:\n",
      "\t\tweight: True\n",
      "\t\tbias: True\n",
      "\tlayer 2:\n",
      "\t\tweight: True\n",
      "\t\tbias: True\n"
     ]
    }
   ],
   "source": [
    "# Save the gradients.\n",
    "g_w1, g_b1 = model.layer_1.weight.grad.data.clone(), model.layer_1.bias.grad.data.clone()\n",
    "g_w2, g_b2 = model.layer_2.weight.grad.data.clone(), model.layer_2.bias.grad.data.clone()\n",
    "# Perform 4 more backward passes.\n",
    "for _ in range(4):\n",
    "    pred_data = model(input_data)\n",
    "    loss = (pred_data - output_data).pow(2).sum()\n",
    "    loss.backward()\n",
    "g_w1_5epochs, g_b1_5epochs = model.layer_1.weight.grad.data.clone(), model.layer_1.bias.grad.data.clone()\n",
    "g_w2_5epochs, g_b2_5epochs = model.layer_2.weight.grad.data.clone(), model.layer_2.bias.grad.data.clone()\n",
    "print('gradients accumulated correctly:')\n",
    "print('\\tlayer 1:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    torch.dist(g_w1*5, g_w1_5epochs), torch.dist(g_b1*5, g_b1_5epochs)\n",
    "))\n",
    "print('\\tlayer 2:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    torch.dist(g_w2*5, g_w2_5epochs), torch.dist(g_b2*5, g_b2_5epochs)\n",
    "))\n",
    "print('gradients match correctly:')\n",
    "print('\\tlayer 1:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    torch.eq(g_w1*5, g_w1_5epochs).all(), torch.eq(g_b1*5, g_b1_5epochs).all()\n",
    "))\n",
    "print('\\tlayer 2:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    torch.eq(g_w2*5, g_w2_5epochs).all(), torch.eq(g_b2*5, g_b2_5epochs).all()\n",
    "))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current gradients\n",
      "\tlayer 1:\n",
      "\t\tweight: tensor([[ -0.2770,   0.0454,  -0.0317,   0.4463,  -0.1871,  -0.0287,   0.6350,\n",
      "          -0.5172,  -0.1002,  -0.5640],\n",
      "        [  1.1041,  -5.3218,   5.5335,   8.1680,   0.7223,  -7.4554,  -4.7095,\n",
      "          -1.5210,   0.4657,  -5.1708],\n",
      "        [ -2.8811,  -5.4991,   5.9507,  16.1975,  -1.9732,  -9.0926,   4.0751,\n",
      "          -9.5416,  -0.9658, -14.4852],\n",
      "        [  4.1099,   9.3287, -10.0497, -25.9778,   2.8215,  15.1561,  -5.1845,\n",
      "          14.6457,   1.3587,  22.8053],\n",
      "        [  6.8560,  -1.4497,   1.1280, -10.4184,   4.6294,   0.2324, -15.8565,\n",
      "          12.5758,   2.4854,  13.4919]])\n",
      "\t\tbias: tensor([ -0.4912,  -4.9239, -13.1039,  20.6798,  11.7236])\n",
      "\tlayer 2:\n",
      "\t\tweight: tensor([[-0.9997,  6.5129, 10.5165, 19.9080,  2.4107],\n",
      "        [ 7.7488, 35.2996, 48.5269, 32.6213, 43.1756]])\n",
      "\t\tbias: tensor([-0.4204, 50.4751])\n",
      "gradients after `.zero_grad()` on the `optimizer`\n",
      "\tlayer 1:\n",
      "\t\tweight: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\t\tbias: tensor([0., 0., 0., 0., 0.])\n",
      "\tlayer 2:\n",
      "\t\tweight: tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "\t\tbias: tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# In order to not accumulate any gradients, call `.zero_grad()` on the `optimizer`\n",
    "print('current gradients')\n",
    "print('\\tlayer 1:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    model.layer_1.weight.grad, model.layer_1.bias.grad\n",
    "))\n",
    "print('\\tlayer 2:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    model.layer_2.weight.grad, model.layer_2.bias.grad\n",
    "))\n",
    "print('gradients after `.zero_grad()` on the `optimizer`')\n",
    "optimizer.zero_grad()\n",
    "print('\\tlayer 1:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    model.layer_1.weight.grad, model.layer_1.bias.grad\n",
    "))\n",
    "print('\\tlayer 2:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    model.layer_2.weight.grad, model.layer_2.bias.grad\n",
    "))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient updates match correctly:\n",
      "\tlayer 1:\n",
      "\t\tweight: 1.0349069157200574e-07\n",
      "\t\tbias: 1.3445788482613352e-08\n",
      "\tlayer 2:\n",
      "\t\tweight: 2.0795969390974278e-08\n",
      "\t\tbias: 5.600729657828651e-09\n"
     ]
    }
   ],
   "source": [
    "# Reset the tracked gradients.\n",
    "optimizer.zero_grad()\n",
    "# Save old weights for reference.\n",
    "w1_before, b1_before = model.layer_1.weight.data.clone(), model.layer_1.bias.data.clone()\n",
    "w2_before, b2_before = model.layer_2.weight.data.clone(), model.layer_2.bias.data.clone()\n",
    "# Perform one forward-backward pass.\n",
    "pred_data = model(input_data)\n",
    "loss = (pred_data - output_data).pow(2).sum()\n",
    "loss.backward()\n",
    "# Save gradients for reference.\n",
    "g_w1, g_b1 = model.layer_1.weight.grad.data.clone(), model.layer_1.bias.grad.data.clone()\n",
    "g_w2, g_b2 = model.layer_2.weight.grad.data.clone(), model.layer_2.bias.grad.data.clone()\n",
    "# In order to actually optimize the weights, call `.step()` on the `optimizer`.\n",
    "optimizer.step()\n",
    "w1_after, b1_after = model.layer_1.weight.data.clone(), model.layer_1.bias.data.clone()\n",
    "w2_after, b2_after = model.layer_2.weight.data.clone(), model.layer_2.bias.data.clone()\n",
    "# Check. NOTE THAT IF THERE IS MOMENTUM OR OTHER MODIFICATIONS TO GRADIENT DESCENT, THE FOLLOWING MATCH WILL FAIL!!\n",
    "print('gradient updates match correctly:')\n",
    "print('\\tlayer 1:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    torch.dist(-1e-3*g_w1, w1_after - w1_before), torch.dist(-1e-3*g_b1, b1_after - b1_before)\n",
    "))\n",
    "print('\\tlayer 2:\\n\\t\\tweight: {}\\n\\t\\tbias: {}'.format(\n",
    "    torch.dist(-1e-3*g_w2, w2_after - w2_before), torch.dist(-1e-3*g_b2, b2_after - b2_before)\n",
    "))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************************************\n",
      "Computation of y with gradients:\n",
      "x:\n",
      "tensor([[ 0.1745,  0.5423],\n",
      "        [ 0.1308, -2.0104],\n",
      "        [ 1.6899,  0.0594]], requires_grad=True)\n",
      "y:\n",
      "tensor([[3.0438e-02, 2.9408e-01],\n",
      "        [1.7108e-02, 4.0419e+00],\n",
      "        [2.8556e+00, 3.5254e-03]], grad_fn=<PowBackward0>)\n",
      "*******************************************************************************\n",
      "Computation of z without gradients:\n",
      "x:\n",
      "tensor([[ 0.1745,  0.5423],\n",
      "        [ 0.1308, -2.0104],\n",
      "        [ 1.6899,  0.0594]], requires_grad=True)\n",
      "z:\n",
      "tensor([[3.0438e-02, 2.9408e-01],\n",
      "        [1.7108e-02, 4.0419e+00],\n",
      "        [2.8556e+00, 3.5254e-03]])\n",
      "*******************************************************************************\n",
      "Computation of w without gradients:\n",
      "x:\n",
      "tensor([[ 0.1745,  0.5423],\n",
      "        [ 0.1308, -2.0104],\n",
      "        [ 1.6899,  0.0594]], requires_grad=True)\n",
      "w:\n",
      "tensor([[3.0438e-02, 2.9408e-01],\n",
      "        [1.7108e-02, 4.0419e+00],\n",
      "        [2.8556e+00, 3.5254e-03]])\n",
      "*******************************************************************************\n",
      "x:\n",
      "tensor([[ 0.1745,  0.5423],\n",
      "        [ 0.1308, -2.0104],\n",
      "        [ 1.6899,  0.0594]], requires_grad=True)\n",
      "x:\n",
      "tensor([[ 0.1745,  0.5423],\n",
      "        [ 0.1308, -2.0104],\n",
      "        [ 1.6899,  0.0594]], requires_grad=True)\n",
      "u:\n",
      "tensor([[3.0438e-02, 2.9408e-01],\n",
      "        [1.7108e-02, 4.0419e+00],\n",
      "        [2.8556e+00, 3.5254e-03]], grad_fn=<PowBackward0>)\n",
      "***************************************\n",
      "x:\n",
      "tensor([[ 0.1745,  0.5423],\n",
      "        [ 0.1308, -2.0104],\n",
      "        [ 1.6899,  0.0594]])\n",
      "x:\n",
      "tensor([[ 0.1745,  0.5423],\n",
      "        [ 0.1308, -2.0104],\n",
      "        [ 1.6899,  0.0594]])\n",
      "u:\n",
      "tensor([[3.0438e-02, 2.9408e-01],\n",
      "        [1.7108e-02, 4.0419e+00],\n",
      "        [2.8556e+00, 3.5254e-03]])\n"
     ]
    }
   ],
   "source": [
    "print('*'*79)\n",
    "x = torch.randn(size=(3, 2), requires_grad=True)  # Or, `.requires_grad_()`\n",
    "y = x**2\n",
    "print('Computation of y with gradients:')\n",
    "print('x:\\n{}\\ny:\\n{}'.format(x, y))\n",
    "print('*'*79)\n",
    "print('Computation of z without gradients:')\n",
    "with torch.no_grad():\n",
    "    z = x**2\n",
    "print('x:\\n{}\\nz:\\n{}'.format(x, z))\n",
    "print('*'*79)\n",
    "print('Computation of w without gradients:')\n",
    "w = x**2\n",
    "# w.requires_grad = False  # THIS WILL NOT WORK!!\n",
    "w = w.detach()  # Creates a COPY of the variable. The copy is not in the tracking graph for backprop.\n",
    "print('x:\\n{}\\nw:\\n{}'.format(x, w))\n",
    "# Setting off the gradients for computation.\n",
    "print('*'*79)\n",
    "print('x:\\n{}'.format(x))\n",
    "with torch.enable_grad():\n",
    "    u = x**2\n",
    "print('x:\\n{}\\nu:\\n{}'.format(x, u))\n",
    "x.requires_grad = False\n",
    "print('*'*39)\n",
    "print('x:\\n{}'.format(x))\n",
    "with torch.enable_grad():\n",
    "    u = x**2\n",
    "print('x:\\n{}\\nu:\\n{}'.format(x, u))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile:\n",
      "-----------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                               Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-----------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                          aten::pow        11.16%     236.558us        12.67%     268.717us      13.436us            20  \n",
      "                        aten::addmm         9.33%     197.754us        12.76%     270.492us      13.525us            20  \n",
      "                            aten::t         7.72%     163.624us        15.12%     320.553us       3.206us           100  \n",
      "                      AddmmBackward         6.85%     145.339us        21.01%     445.532us      22.277us            20  \n",
      "                          aten::mul         5.85%     124.023us         8.85%     187.583us       6.253us            30  \n",
      "                          aten::sum         5.64%     119.642us         7.93%     168.057us       5.602us            30  \n",
      "    torch::autograd::AccumulateGrad         4.89%     103.749us         8.23%     174.544us       2.909us            60  \n",
      "                    aten::transpose         4.75%     100.712us         7.40%     156.929us       1.569us           100  \n",
      "                           aten::mm         4.47%      94.754us         5.83%     123.699us       3.092us            40  \n",
      "                   aten::as_strided         3.66%      77.662us         3.66%      77.662us       0.555us           140  \n",
      "                         aten::add_         3.34%      70.795us         3.34%      70.795us       1.180us            60  \n",
      "                        aten::copy_         3.26%      69.159us         3.26%      69.159us       1.383us            50  \n",
      "                       aten::linear         2.90%      61.426us        20.27%     429.747us      21.487us            20  \n",
      "                        aten::empty         2.73%      57.838us         2.73%      57.838us       0.723us            80  \n",
      "                       PowBackward0         2.58%      54.734us        16.99%     360.240us      36.024us            10  \n",
      "                       aten::expand         2.06%      43.626us         2.84%      60.281us       2.009us            30  \n",
      "                          aten::sub         2.02%      42.739us         2.02%      42.739us       4.274us            10  \n",
      "                    aten::clamp_min         1.87%      39.746us         3.19%      67.708us       3.385us            20  \n",
      "                           aten::to         1.75%      37.036us         3.30%      69.967us       1.749us            40  \n",
      "                         aten::relu         1.62%      34.438us         3.83%      81.219us       8.122us            10  \n",
      "                aten::empty_strided         1.30%      27.553us         1.30%      27.553us       0.918us            30  \n",
      "                         aten::view         1.19%      25.289us         1.19%      25.289us       1.264us            20  \n",
      "           aten::threshold_backward         1.13%      24.029us         1.13%      24.029us       2.403us            10  \n",
      "                      ReluBackward0         1.12%      23.751us         2.25%      47.780us       4.778us            10  \n",
      "                        aten::fill_         1.12%      23.653us         1.12%      23.653us       0.591us            40  \n",
      "                       SubBackward0         0.97%      20.608us         5.39%     114.330us      11.433us            10  \n",
      "                          aten::neg         0.83%      17.657us         0.83%      17.657us       1.766us            10  \n",
      "                          TBackward         0.81%      17.251us         3.45%      73.059us       3.653us            20  \n",
      "                    aten::ones_like         0.79%      16.669us         2.02%      42.875us       4.287us            10  \n",
      "                  aten::result_type         0.62%      13.126us         0.62%      13.126us       0.328us            40  \n",
      "                   aten::empty_like         0.61%      12.898us         1.15%      24.320us       2.432us            10  \n",
      "                         aten::conj         0.55%      11.613us         0.55%      11.613us       0.232us            50  \n",
      "                       SumBackward0         0.52%      11.087us         1.81%      38.363us       3.836us            10  \n",
      "-----------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.121ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Autograd profiler.\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "is_cuda = torch.cuda.is_available()\n",
    "# Perform computations.\n",
    "with torch.autograd.profiler.profile(use_cuda=is_cuda) as profile:\n",
    "    for _ in range(10):\n",
    "        pred_data = model(input_data)\n",
    "        loss = (pred_data - output_data).pow(2).sum()\n",
    "        loss.backward()\n",
    "# Print the analysis.\n",
    "print('profile:\\n{}'.format(profile.key_averages().table(sort_by='self_cpu_time_total')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "tensor([[-0.2604,  1.1346, -1.1783, -1.6933, -0.1709,  1.5809,  1.0596,  0.2748,\n",
      "         -0.1083,  1.0456],\n",
      "        [-0.0447,  0.2723, -0.1933, -0.9828, -0.5130, -1.9057,  1.0917, -0.7817,\n",
      "         -0.6409, -1.6574],\n",
      "        [ 0.5639, -0.0925,  0.0646, -0.9086,  0.3809,  0.0585, -1.2927,  1.0529,\n",
      "          0.2041,  1.1482],\n",
      "        [ 0.3414, -0.1821, -0.6738,  0.4276, -0.6007, -0.5217,  0.0145,  0.2583,\n",
      "          0.0299, -2.6737]], requires_grad=True)\n",
      "d Loss / d Inputs\n",
      "\tshape: torch.Size([4, 2, 4, 10])\n",
      "\tdata: tensor([[[[-1.6328, -0.7112, -0.5510, -0.6306,  0.0173, -0.6822,  0.0381,\n",
      "           -1.0199, -0.5788, -0.7113],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.8130,  0.2959,  0.4006,  0.9496,  0.6703,  0.6561,  0.8152,\n",
      "            0.1536,  0.8482,  0.9890],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000],\n",
      "          [ 0.5901,  0.1781,  0.2561,  0.3296,  0.0248,  0.2706,  0.0598,\n",
      "            0.3259,  0.2502,  0.2442],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000],\n",
      "          [ 1.2733,  0.5588,  0.5892,  1.5686,  1.2514,  1.1074,  1.4723,\n",
      "            0.1745,  1.4720,  1.7994],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000]]]])\n",
      "*******************************************************************************\n",
      "jacobian:\n",
      "\tinput 1:\n",
      "\t\tshape: torch.Size([3, 4, 3, 4])\n",
      "\t\tdata: tensor([[[[1., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 1., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 1.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0.],\n",
      "          [1., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 1., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 1., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 1.],\n",
      "          [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 1., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 1., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 1.]]]])\n",
      "\tinput 2:\n",
      "\t\tshape: torch.Size([3, 4, 4])\n",
      "\t\tdata: tensor([[[-1.8845,  0.0000, -0.0000,  0.0000],\n",
      "         [-0.0000,  2.6827, -0.0000,  0.0000],\n",
      "         [-0.0000,  0.0000, -1.8584,  0.0000],\n",
      "         [-0.0000,  0.0000, -0.0000,  0.9862]],\n",
      "\n",
      "        [[-1.8845,  0.0000, -0.0000,  0.0000],\n",
      "         [-0.0000,  2.6827, -0.0000,  0.0000],\n",
      "         [-0.0000,  0.0000, -1.8584,  0.0000],\n",
      "         [-0.0000,  0.0000, -0.0000,  0.9862]],\n",
      "\n",
      "        [[-1.8845,  0.0000, -0.0000,  0.0000],\n",
      "         [-0.0000,  2.6827, -0.0000,  0.0000],\n",
      "         [-0.0000,  0.0000, -1.8584,  0.0000],\n",
      "         [-0.0000,  0.0000, -0.0000,  0.9862]]])\n"
     ]
    }
   ],
   "source": [
    "# Computing Jacobians and Hessians for a computation.\n",
    "# For this, we need to wrap our computation as a function.\n",
    "# noinspection PyMissingOrEmptyDocstring\n",
    "def calculate_loss(input_data_):\n",
    "    pred_data_ = model(input_data_)\n",
    "    return (pred_data_ - output_data)**2\n",
    "print('input:\\n{}'.format(input_data))\n",
    "inputs = (input_data, )\n",
    "jacobian = torch.autograd.functional.jacobian(calculate_loss, inputs)\n",
    "print('d Loss / d Inputs')\n",
    "print('\\tshape: {}'.format(jacobian[0].shape))\n",
    "print('\\tdata: {}'.format(jacobian[0]))\n",
    "# A simple example.\n",
    "print('*'*79)\n",
    "# noinspection PyMissingOrEmptyDocstring\n",
    "def fn(input_1, input_2):\n",
    "    return input_1 + input_2**2\n",
    "input_1_data = torch.Tensor(3, 4).normal_().requires_grad_()\n",
    "input_2_data = torch.Tensor(4, ).normal_().requires_grad_()\n",
    "input_pair = (input_1_data, input_2_data)\n",
    "jacobian = torch.autograd.functional.jacobian(fn, input_pair)\n",
    "print('jacobian:')\n",
    "print('\\tinput 1:')\n",
    "print('\\t\\tshape: {}'.format(jacobian[0].shape))\n",
    "print('\\t\\tdata: {}'.format(jacobian[0]))\n",
    "print('\\tinput 2:')\n",
    "print('\\t\\tshape: {}'.format(jacobian[1].shape))\n",
    "print('\\t\\tdata: {}'.format(jacobian[1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}